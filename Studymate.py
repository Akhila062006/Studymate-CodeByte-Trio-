# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fnuf7Fmr3jJrihEsHsCWxtb7C-ACJmLr
"""

!pip install gradio pymupdf sentence-transformers faiss-cpu transformers accelerate -q

import fitz
import faiss
import numpy as np
import gradio as gr
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

# -----------------------------------------------------
# 1. TEXT EXTRACTION
# -----------------------------------------------------
def extract_text_from_pdf(pdf_file):
    """Extracts text from a PDF file using PyMuPDF."""
    try:
        pdf_path = pdf_file.name  # Gradio gives NamedTemporaryFile
        doc = fitz.open(pdf_path)
    except Exception:
        return ""

    text = ""
    for page in doc:
        text += page.get_text()

    return text


# -----------------------------------------------------
# 2. CHUNKING
# -----------------------------------------------------
def chunk_text(text, chunk_size=400):
    words = text.split()
    if len(words) == 0:
        return []
    return [" ".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]


# -----------------------------------------------------
# 3. EMBEDDING + FAISS INDEX
# -----------------------------------------------------
embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

faiss_index = None
text_chunks = []


def process_pdfs(pdf_files):
    """Extract, chunk, embed and index PDFs."""
    global faiss_index, text_chunks

    if not isinstance(pdf_files, list):
        pdf_files = [pdf_files]

    all_text = ""

    for pdf in pdf_files:
        t = extract_text_from_pdf(pdf)
        all_text += t

    if len(all_text.strip()) == 0:
        return "‚ùå No readable text found. PDF might be scanned or empty."

    text_chunks = chunk_text(all_text)
    if len(text_chunks) == 0:
        return "‚ùå PDF contains no text."

    embeddings = embedding_model.encode(text_chunks)
    dim = embeddings.shape[1]

    faiss_index = faiss.IndexFlatL2(dim)
    faiss_index.add(np.array(embeddings))

    return f"‚úÖ Successfully processed {len(pdf_files)} PDF(s). Indexed {len(text_chunks)} text chunks."


# -----------------------------------------------------
# 4. LOAD IBM GRANITE MODEL (HuggingFace ‚Äì no key needed)
# -----------------------------------------------------
model_name = "ibm-granite/granite-3.3-2b-instruct"

print("‚è≥ Loading IBM Granite model‚Ä¶")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype="auto",
    device_map="auto"
)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=200
)


# -----------------------------------------------------
# 5. SEMANTIC SEARCH
# -----------------------------------------------------
def semantic_search(query, top_k=3):
    if faiss_index is None:
        return None

    q_embed = embedding_model.encode([query])
    distances, indices = faiss_index.search(np.array(q_embed), top_k)

    context = "\n\n".join([text_chunks[i] for i in indices[0]])
    return context


# -----------------------------------------------------
# 6. ANSWERING USER QUERIES (Q&A)
# -----------------------------------------------------
def answer_query(query):
    if faiss_index is None:
        return "‚ö†Ô∏è Please upload and process PDF(s) first."

    context = semantic_search(query)
    if not context:
        return "‚ö†Ô∏è No context retrieved."

    prompt = (
        "You are StudyMate, an academic assistant.\n"
        "Use ONLY the context below to answer the question.\n\n"
        f"Context:\n{context}\n\n"
        f"Question: {query}\n\n"
        "Answer:"
    )

    response = pipe(prompt)[0]["generated_text"]
    answer = response[len(prompt):].strip()

    return answer


# -----------------------------------------------------
# 7. GRADIO UI
# -----------------------------------------------------
with gr.Blocks() as app:
    gr.Markdown("# üìò StudyMate ‚Äì AI PDF Q&A (IBM Granite)")

    files = gr.File(label="üìÇ Upload PDF files", file_count="multiple", file_types=[".pdf"])
    process_btn = gr.Button("üì• Process PDFs")
    status = gr.Textbox(label="Processing Status")

    process_btn.click(process_pdfs, inputs=files, outputs=status)

    gr.Markdown("### ‚ùì Ask a Question")
    query = gr.Textbox(label="Enter your question here‚Ä¶")
    ask_btn = gr.Button("üí¨ Get Answer")
    answer = gr.Textbox(label="Answer")

    ask_btn.click(answer_query, inputs=query, outputs=answer)

app.launch()